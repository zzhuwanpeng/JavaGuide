## 业务介绍
1. 采集音频来源：主播上传，广播，直播，抓取；转码入库；主播站，媒资系统
2. 用户收听：app，车载，第三方api调用
3. 业务：收听，直播，付费，积分，开放运营，智能电台。
4. 数据量 3000音频，用户量百万，日活用户50万。

## 我主导进行了三方面的架构设计：
1. 智能电台
   千人千面，通过内存，redis等提高性能
   通过akka系统进一步解决用户刷新播单的性能问题。
2. CQRS
   解决读写分离，数据不一致的问题
   采用cannal作为数据同步手段，然后  
3. 基于TOB的复杂业务，采用栏目组+运营位+使用范围的模式，解决了多厂家展示，不同端展示的需求；
   原有设计按页面进行开发，迭代太慢，同一个音频在不同配置下无法解决试用范围的问题。
   涉及风控，源控制，时段展示，头图等各个方面；并形成开放平台。车场可以直接对接。



##
业务概念：
栏目，分类 （适用范围）
专辑，直播，电台，广播
碎片
标签，风控标签，付费信息：风控，主播，用户信息

流程：
手机，车载，k-radio，openapi，小程序
主播上传，媒资上传
付费，审核
智能电台（品牌电台）：台宣，碎片，广播，直播，时效电台，权重电台
直播，广播
灵活配置运营位



#  数据同步-cannal
databus: 同步分片(线程)来提高性能，每行一个id用于分片，按行保证顺序性（可以不按binlog顺序）

## cannal
Server端获取binlog，封装成CannalMessage,里面是对应的Entry
Client **订阅**消息，在代码中处理关心的id，并发的进行处理
通过zk完成高可用，消费记录也记录在offset端。

多线程下的顺序性：
先确保每个client只能分配到固定的ID，（或者直接按库表分）
机器内部有一个阻塞队列记录了各个批次号：LinkedBlockingQueue<Long> batchIds，这样批次号必须顺序消费

幂等性：insert重复更新es

使用ES的原因：


## 智能电台
1. 每个电台有自己的编排，编排按clock切换，如早中晚，节假日时段。同一编排内有不同的轮次，每个轮次按编排位形成不同的节目
编排位有不同的类型，如台宣，个推位，直播，普通音频节目等，一般节目可以从分类或标签维度，车型，地域等维度取出，可以按专辑的分值，上线时间一轮轮的取出。
2. 以上形成了公共播单，根据去重，个性化推荐形成个人播单
3. 难点在于：每次更新碎片或者专辑，标签等刷新计算量非常大，一次更新需要多台机器。

akka：
+ 解决了并发顺序性的问题，比如多个音频同时更新播单，在多线程的环境容易错乱，使用actor可以避免这样的问题，同时基于邮箱可以归并一些无效的更新（自定义邮箱）  
+ 任务管理，容错
+ akka的高并发的性能，异步非阻塞


+ storm接收收听历史，存入redis


数据量：






